{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper as utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, _, testX, _ = utils.load_data('reviews_rus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = trainX + testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [list(filter(lambda x: x != '' and len(x) > 1, re.split('[^а-я]', line.lower()))  ) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for sent in corpus for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['дизайн', 'приятные', 'габариты', 'удобно', 'лежит', 'руке', 'бесплатные', 'приложения', 'игры', 'хотя']\n",
      "1527876\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_stops = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for word in corpus if word not in rus_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "Build the dictionary and replace rare words with `UNK`token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, i2w = utils.build_dataset(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [23, 0, 0, 27, 95, 65, 0, 29, 59, 21] ['дизайн', 'UNK', 'UNK', 'удобно', 'лежит', 'руке', 'UNK', 'приложения', 'игры', 'хотя']\n"
     ]
    }
   ],
   "source": [
    "print('Sample data', data[:10], [i2w[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(batch_size, num_skips, wing):\n",
    "    x, y = [],[]\n",
    "    assert batch_size <= len(data) - 2*wing\n",
    "    assert batch_size % num_skips == 0\n",
    "\n",
    "    target_idcs = random.sample(range(wing,len(data)-wing), batch_size//num_skips)\n",
    "    for t_idx in target_idcs:\n",
    "        for _ in range(num_skips):\n",
    "            c_idx = random.sample( list(range (t_idx - wing, t_idx) ) + list( range (t_idx+1, t_idx+wing+1) ),1 )\n",
    "            x.append(data[t_idx])\n",
    "            y.append(data[c_idx[0]])\n",
    "\n",
    "    x = np.array(x, dtype=np.int32)\n",
    "    y = np.array(y, dtype=np.int32).reshape(-1,1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 кнопка -> 81 тормозит\n",
      "129 кнопка -> 0 UNK\n",
      "46 пользуюсь -> 512 играть\n",
      "46 пользуюсь -> 512 играть\n",
      "0 UNK -> 60 крышка\n",
      "0 UNK -> 24 быстро\n",
      "400 свой -> 0 UNK\n",
      "400 свой -> 1 телефон\n"
     ]
    }
   ],
   "source": [
    "batch, labels = gen_batch(batch_size=8, num_skips=2, wing=2)\n",
    "for b, l in zip(batch,labels): # range(8):\n",
    "    print(b, i2w[b], '->', l[0],\n",
    "          i2w[l[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'reviews_ru_log'\n",
    "if not os.path.exists(log_dir):\n",
    "    print('making the directory.')\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/miniconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ivan/miniconda3/envs/ml/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 50  # Dimension of the embedding vector.\n",
    "skip_window = 2  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 32  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit\n",
    "# the validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 4  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size],\n",
    "                                                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                             biases=nce_biases,\n",
    "                                             labels=train_labels,\n",
    "                                             inputs=embed,\n",
    "                                             num_sampled=num_sampled,\n",
    "                                             num_classes=vocab_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "#         optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  95.57229614257812\n",
      "Nearest to нужно: шустро, жутко, обратно, сборка,\n",
      "Nearest to года: столько, новые, чехле, стоит,\n",
      "Nearest to работы: лежит, плохо, слышно, возможности,\n",
      "Nearest to мало: некоторых, жизни, идет, пару,\n",
      "Average loss at step  2000 :  6.79695642375946\n",
      "Average loss at step  4000 :  3.7111902314424516\n",
      "Average loss at step  6000 :  3.6890621777772905\n",
      "Average loss at step  8000 :  3.6613554170131684\n",
      "Average loss at step  10000 :  3.6440309468507768\n",
      "Average loss at step  12000 :  3.6376683564186094\n",
      "Average loss at step  14000 :  3.6353981994390487\n",
      "Average loss at step  16000 :  3.6256597045660017\n",
      "Average loss at step  18000 :  3.611668218612671\n",
      "Average loss at step  20000 :  3.605453843832016\n",
      "Nearest to нужно: UNK, телефоны, начинает, это,\n",
      "Nearest to года: год, стал, двух, телефоном,\n",
      "Nearest to работы: это, выше, использую, лежит,\n",
      "Nearest to мало: внутренней, слабый, зарядку, UNK,\n",
      "Average loss at step  22000 :  3.603331997990608\n",
      "Average loss at step  24000 :  3.5962936503887177\n",
      "Average loss at step  26000 :  3.603208481311798\n",
      "Average loss at step  28000 :  3.59458600795269\n",
      "Average loss at step  30000 :  3.583257088303566\n",
      "Average loss at step  32000 :  3.571854228377342\n",
      "Average loss at step  34000 :  3.5661962920427324\n",
      "Average loss at step  36000 :  3.5846347068548203\n",
      "Average loss at step  38000 :  3.557336021065712\n",
      "Average loss at step  40000 :  3.562099813580513\n",
      "Nearest to нужно: это, играет, телефоны, UNK,\n",
      "Nearest to года: год, месяца, стал, двух,\n",
      "Nearest to работы: выше, использую, привыкнуть, хорошего,\n",
      "Nearest to мало: мб, внутренней, слабый, памяти,\n",
      "Average loss at step  42000 :  3.571175023555756\n",
      "Average loss at step  44000 :  3.5646254435777665\n",
      "Average loss at step  46000 :  3.560360151410103\n",
      "Average loss at step  48000 :  3.547679096221924\n",
      "Average loss at step  50000 :  3.550171652674675\n",
      "Average loss at step  52000 :  3.5546089116334914\n",
      "Average loss at step  54000 :  3.5399909884929657\n",
      "Average loss at step  56000 :  3.5491753042936325\n",
      "Average loss at step  58000 :  3.5425625846385955\n",
      "Average loss at step  60000 :  3.5531789540052414\n",
      "Nearest to нужно: UNK, интернетом, играет, это,\n",
      "Nearest to года: год, месяца, полгода, лет,\n",
      "Nearest to работы: использую, аккумулятора, уровне, удобство,\n",
      "Nearest to мало: внутренней, слабый, мб, программ,\n",
      "Average loss at step  62000 :  3.5316769979000093\n",
      "Average loss at step  64000 :  3.541177964568138\n",
      "Average loss at step  66000 :  3.5340484297275543\n",
      "Average loss at step  68000 :  3.542428406834602\n",
      "Average loss at step  70000 :  3.532416990876198\n",
      "Average loss at step  72000 :  3.53614989733696\n",
      "Average loss at step  74000 :  3.5305391966104507\n",
      "Average loss at step  76000 :  3.512596039056778\n",
      "Average loss at step  78000 :  3.5282602689266205\n",
      "Average loss at step  80000 :  3.5357914584875108\n",
      "Nearest to нужно: понятно, просто, ставить, невозможно,\n",
      "Nearest to года: год, месяца, лет, полгода,\n",
      "Nearest to работы: удобство, уровне, использования, аккумулятора,\n",
      "Nearest to мало: внутренней, встроенной, оперативной, мб,\n",
      "Average loss at step  82000 :  3.5258236054182053\n",
      "Average loss at step  84000 :  3.5287344821691513\n",
      "Average loss at step  86000 :  3.537755716443062\n",
      "Average loss at step  88000 :  3.520828022480011\n",
      "Average loss at step  90000 :  3.53604246866703\n",
      "Average loss at step  92000 :  3.5183351490497587\n",
      "Average loss at step  94000 :  3.518121744275093\n",
      "Average loss at step  96000 :  3.520229192852974\n",
      "Average loss at step  98000 :  3.5225065670013427\n",
      "Average loss at step  100000 :  3.5219576843976976\n",
      "Nearest to нужно: понятно, приходится, интернетом, включить,\n",
      "Nearest to года: месяца, год, полгода, лет,\n",
      "Nearest to работы: удобство, времени, пользуюсь, уровне,\n",
      "Nearest to мало: внутренней, оперативной, встроенной, мб,\n",
      "Average loss at step  102000 :  3.5178418402671814\n",
      "Average loss at step  104000 :  3.5165155571699143\n",
      "Average loss at step  106000 :  3.517673905611038\n",
      "Average loss at step  108000 :  3.511048514842987\n",
      "Average loss at step  110000 :  3.5177951122522355\n",
      "Average loss at step  112000 :  3.512717043161392\n",
      "Average loss at step  114000 :  3.50910951256752\n",
      "Average loss at step  116000 :  3.51251085793972\n",
      "Average loss at step  118000 :  3.515142302751541\n",
      "Average loss at step  120000 :  3.517094710826874\n",
      "Nearest to нужно: приходится, понятно, возможность, иначе,\n",
      "Nearest to года: месяца, год, лет, месяц,\n",
      "Nearest to работы: быстродействие, удобство, производительность, работе,\n",
      "Nearest to мало: встроенной, оперативной, внутренней, нету,\n",
      "Average loss at step  122000 :  3.50937147629261\n",
      "Average loss at step  124000 :  3.4969316591024397\n",
      "Average loss at step  126000 :  3.503854877591133\n",
      "Average loss at step  128000 :  3.509815682888031\n",
      "Average loss at step  130000 :  3.5054523258209227\n",
      "Average loss at step  132000 :  3.5044963289499282\n",
      "Average loss at step  134000 :  3.497469456911087\n",
      "Average loss at step  136000 :  3.5031031147241594\n",
      "Average loss at step  138000 :  3.4897543182373045\n",
      "Average loss at step  140000 :  3.492224916100502\n",
      "Nearest to нужно: приходится, понятно, возможность, иначе,\n",
      "Nearest to года: год, месяца, полгода, месяц,\n",
      "Nearest to работы: быстродействие, производительность, удобство, скорость,\n",
      "Nearest to мало: внутренней, встроенной, оперативной, мб,\n",
      "Average loss at step  142000 :  3.4946661833524706\n",
      "Average loss at step  144000 :  3.49468963432312\n",
      "Average loss at step  146000 :  3.49871336877346\n",
      "Average loss at step  148000 :  3.4991402838230132\n",
      "Average loss at step  150000 :  3.5003958070278167\n",
      "Average loss at step  152000 :  3.495395795583725\n",
      "Average loss at step  154000 :  3.4884413702487946\n",
      "Average loss at step  156000 :  3.4939019070863724\n",
      "Average loss at step  158000 :  3.4811715520620345\n",
      "Average loss at step  160000 :  3.49348722755909\n",
      "Nearest to нужно: приходится, понятно, возможность, каждый,\n",
      "Nearest to года: год, месяца, полгода, месяц,\n",
      "Nearest to работы: работе, быстродействие, удобство, UNK,\n",
      "Nearest to мало: внутренней, встроенной, оперативной, мб,\n",
      "Average loss at step  162000 :  3.4963980548381803\n",
      "Average loss at step  164000 :  3.4927378743886948\n",
      "Average loss at step  166000 :  3.495571514725685\n",
      "Average loss at step  168000 :  3.488035837173462\n",
      "Average loss at step  170000 :  3.4926176344156263\n",
      "Average loss at step  172000 :  3.487623771905899\n",
      "Average loss at step  174000 :  3.486523994922638\n",
      "Average loss at step  176000 :  3.4843400859832765\n",
      "Average loss at step  178000 :  3.4947153780460356\n",
      "Average loss at step  180000 :  3.4800187005996706\n",
      "Nearest to нужно: приходится, иначе, каждый, возможность,\n",
      "Nearest to года: год, месяца, полгода, месяц,\n",
      "Nearest to работы: производительность, скорость, быстродействие, железо,\n",
      "Nearest to мало: внутренней, оперативной, встроенной, нету,\n",
      "Average loss at step  182000 :  3.4987474689483644\n",
      "Average loss at step  184000 :  3.4861881824731826\n",
      "Average loss at step  186000 :  3.4889188681840895\n",
      "Average loss at step  188000 :  3.4747936408519746\n",
      "Average loss at step  190000 :  3.493279751062393\n",
      "Average loss at step  192000 :  3.482960509419441\n",
      "Average loss at step  194000 :  3.4797702916860582\n",
      "Average loss at step  196000 :  3.4846637758016588\n",
      "Average loss at step  198000 :  3.494291174173355\n",
      "Average loss at step  200000 :  3.4826074632406234\n",
      "Nearest to нужно: приходится, возможность, иначе, понятно,\n",
      "Nearest to года: месяца, год, полгода, месяц,\n",
      "Nearest to работы: скорость, работа, аккумулятора, автономность,\n",
      "Nearest to мало: внутренней, оперативной, встроенной, нету,\n",
      "Average loss at step  202000 :  3.48536014175415\n",
      "Average loss at step  204000 :  3.4831619831323626\n",
      "Average loss at step  206000 :  3.489663078546524\n",
      "Average loss at step  208000 :  3.483637524008751\n",
      "Average loss at step  210000 :  3.4769495459794997\n",
      "Average loss at step  212000 :  3.481949998021126\n",
      "Average loss at step  214000 :  3.480433917284012\n",
      "Average loss at step  216000 :  3.481850136876106\n",
      "Average loss at step  218000 :  3.479997951745987\n",
      "Average loss at step  220000 :  3.481606759905815\n",
      "Nearest to нужно: приходится, возможность, иначе, файлы,\n",
      "Nearest to года: месяца, год, лет, полгода,\n",
      "Nearest to работы: скорость, автономность, интернета, пользования,\n",
      "Nearest to мало: внутренней, мб, оперативной, встроенной,\n",
      "Average loss at step  222000 :  3.4778490006923675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  224000 :  3.481088665604591\n",
      "Average loss at step  226000 :  3.479821900486946\n",
      "Average loss at step  228000 :  3.482370329380035\n",
      "Average loss at step  230000 :  3.4886057900190353\n",
      "Average loss at step  232000 :  3.4772582433223724\n",
      "Average loss at step  234000 :  3.4817662984132767\n",
      "Average loss at step  236000 :  3.4746250557899474\n",
      "Average loss at step  238000 :  3.481174899339676\n",
      "Average loss at step  240000 :  3.4814167448282243\n",
      "Nearest to нужно: приходится, возможность, иначе, понятно,\n",
      "Nearest to года: месяца, год, полгода, лет,\n",
      "Nearest to работы: автономность, скорость, работе, аккумулятора,\n",
      "Nearest to мало: встроенной, нету, внутренней, количество,\n",
      "Average loss at step  242000 :  3.477795501470566\n",
      "Average loss at step  244000 :  3.4717944071292877\n",
      "Average loss at step  246000 :  3.479409617304802\n",
      "Average loss at step  248000 :  3.474868056178093\n",
      "Average loss at step  250000 :  3.4774734518527985\n",
      "Average loss at step  252000 :  3.4704763680696487\n",
      "Average loss at step  254000 :  3.483591309070587\n",
      "Average loss at step  256000 :  3.482362477183342\n",
      "Average loss at step  258000 :  3.47256904900074\n",
      "Average loss at step  260000 :  3.477575778245926\n",
      "Nearest to нужно: приходится, возможность, приходилось, невозможно,\n",
      "Nearest to года: год, месяца, лет, полгода,\n",
      "Nearest to работы: автономность, скорость, батареи, аккумулятора,\n",
      "Nearest to мало: встроенной, количество, внутренней, нету,\n",
      "Average loss at step  262000 :  3.4774655510187147\n",
      "Average loss at step  264000 :  3.4820690524578093\n",
      "Average loss at step  266000 :  3.4692822024822236\n",
      "Average loss at step  268000 :  3.4686111878156662\n",
      "Average loss at step  270000 :  3.4747091690301897\n",
      "Average loss at step  272000 :  3.4701628165245055\n",
      "Average loss at step  274000 :  3.4757076758146286\n",
      "Average loss at step  276000 :  3.4703664005994797\n",
      "Average loss at step  278000 :  3.4701209493875504\n",
      "Average loss at step  280000 :  3.463126672625542\n",
      "Nearest to нужно: приходится, возможность, иначе, приходилось,\n",
      "Nearest to года: год, месяца, лет, неделю,\n",
      "Nearest to работы: автономность, аккумулятора, батареи, пользования,\n",
      "Nearest to мало: встроенной, внутренней, количество, нету,\n",
      "Average loss at step  282000 :  3.473651533842087\n",
      "Average loss at step  284000 :  3.4790499331951144\n",
      "Average loss at step  286000 :  3.4736146297454833\n",
      "Average loss at step  288000 :  3.4742153633832933\n",
      "Average loss at step  290000 :  3.471869211554527\n",
      "Average loss at step  292000 :  3.4664901905059815\n",
      "Average loss at step  294000 :  3.4623390365839004\n",
      "Average loss at step  296000 :  3.4741086102724075\n",
      "Average loss at step  298000 :  3.4724643689394\n"
     ]
    }
   ],
   "source": [
    "num_steps = 300000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(logdir=log_dir, graph=session.graph)\n",
    "    \n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = gen_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned\n",
    "        # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "        # the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss], \n",
    "                                           feed_dict=feed_dict,\n",
    "                                           run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000\n",
    "            # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 20000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = i2w[valid_examples[i]]\n",
    "                top_k = 4  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = i2w[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(os.path.join(log_dir, log_dir, 'metadata.tsv'), 'w') as f:\n",
    "        for i in range(vocab_size):\n",
    "            f.write(i2w[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    save_path = os.path.join(log_dir, 'model.ckpt')\n",
    "#     print(save_path)\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in\n",
    "    # TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    projector_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "#     print(projector_path)\n",
    "    embedding_conf.metadata_path = projector_path\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(log_dir))\n",
    "    embed_mat = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embed_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
